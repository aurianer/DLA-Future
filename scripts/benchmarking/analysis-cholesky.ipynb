{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from parsers import generic, dlaf, slate, dplasma, scalapack\n",
    "\n",
    "colors_default = plt.cm.get_cmap('tab10').colors\n",
    "\n",
    "LIB_COLOR = {\n",
    "    'dlaf': colors_default[2], \n",
    "    'slate': colors_default[0],\n",
    "    'dplasma': colors_default[1],\n",
    "    'scalapack': colors_default[4],\n",
    "    'prototype': 'k',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data from filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_db = {}\n",
    "for slurm_db_filepath in [\n",
    "    'slurm.db',\n",
    "    '/Users/ialberto/workspace/benchmarks/20200619-benchmark-hpxmod_and_mpi-executor/raw_data/slurm.db',\n",
    "        ]:\n",
    "    data = generic.parse_slurm_db(slurm_db_filepath)\n",
    "    slurm_db = {**slurm_db, **data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "campaigns = {\n",
    "    'post': 'raw_data/post/cholesky/',\n",
    "    '1.5.0': 'raw_data/porting',\n",
    "#    'ref': '/Users/ialberto/workspace/benchmarks/20200619-benchmark-hpxmod_and_mpi-executor/raw_data',\n",
    "    'ref-mar20': '/Users/ialberto/workspace/benchmarks/20200317-DLAF_miniappcholesky-pre/raw_data',\n",
    "}\n",
    "\n",
    "jobs_reports = []\n",
    "\n",
    "df = pd.DataFrame([], columns=[\n",
    "    'library',\n",
    "    'benchmark_lib',\n",
    "    'run_index',\n",
    "    'm',\n",
    "    'mb',\n",
    "    'grid_rows',\n",
    "    'grid_cols',\n",
    "    'time',\n",
    "    'performance',\n",
    "    'id',\n",
    "    'campaign',\n",
    "])\n",
    "\n",
    "for campaign_name, workdir in campaigns.items():\n",
    "    for entry in os.listdir(workdir):\n",
    "        m = re.match('(\\d+)J\\d+_\\d+', entry)\n",
    "\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        jobid, = m.groups()\n",
    "        jobdata = slurm_db[jobid]\n",
    "        jobname = jobdata['jobname']\n",
    "\n",
    "        first_seen = jobid not in jobs_reports\n",
    "        jobs_reports.append(jobid)\n",
    "\n",
    "        if (jobdata['state'] != 'COMPLETED'):\n",
    "            if first_seen:\n",
    "                display(f'WARNING {jobid} {jobname} {jobdata[\"state\"]}')\n",
    "            continue\n",
    "\n",
    "        # select parser\n",
    "        if re.search(r'.*slate.*', jobname):\n",
    "            benchmark_lib = 'slate'\n",
    "            parser = slate.parse_cholesky\n",
    "        elif re.search(r'.*dplasma.*', jobname):\n",
    "            benchmark_lib = 'dplasma'\n",
    "            parser = dplasma.parse_cholesky\n",
    "        elif re.search(r'.*scalapack.*', jobname):\n",
    "            benchmark_lib = 'scalapack'\n",
    "            parser = scalapack.parse_cholesky\n",
    "        else:\n",
    "            benchmark_lib = 'dlaf'\n",
    "            parser = dlaf.parse_cholesky\n",
    "\n",
    "        current_run = os.path.join(workdir, entry)\n",
    "        for task_entry in os.listdir(current_run):\n",
    "            if not task_entry.endswith('.out'):\n",
    "                continue\n",
    "\n",
    "            out_filepath = os.path.join(current_run, task_entry)\n",
    "            try:\n",
    "                current_data = parser(out_filepath)\n",
    "            except Exception as e:\n",
    "                display(f'ERROR {out_filepath} {e}')\n",
    "                continue\n",
    "\n",
    "            if len(current_data[1]) == 0:\n",
    "                display(f'No data in {out_filepath}')\n",
    "                continue\n",
    "\n",
    "            current_df = pd.DataFrame(current_data[1], columns=[\n",
    "                'run_index',\n",
    "                'm',\n",
    "                'mb',\n",
    "                'grid_rows',\n",
    "                'grid_cols',\n",
    "                'time',\n",
    "                'performance',\n",
    "            ])\n",
    "\n",
    "            # do not plot results with more than 64 nodes\n",
    "            if (jobdata[\"nodes\"] > 64):\n",
    "                continue\n",
    "\n",
    "            current_df['campaign'] = campaign_name\n",
    "            current_df['library'] = benchmark_lib\n",
    "            current_df['nodes'] = jobdata[\"nodes\"]\n",
    "            current_df['ranks_per_node'] = current_df['grid_rows'] * current_df['grid_cols'] / current_df[\"nodes\"]\n",
    "            current_df['benchmark_lib'] = benchmark_lib + \"|\" + str(int(current_df['ranks_per_node'].iloc[0]))\n",
    "            current_df['performance_per_node'] = current_df['performance'] / current_df['nodes']\n",
    "\n",
    "            current_df['id'] = entry\n",
    "\n",
    "            df = df.append(current_df)\n",
    "\n",
    "display(\"LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs = df\\\n",
    ".loc[df['run_index'] != 0]\\\n",
    ".groupby(['library', 'm', 'mb', 'nodes', 'ranks_per_node', 'benchmark_lib', 'campaign'])\\\n",
    ".agg(\n",
    "    p_mean=(\"performance\", 'mean'),\n",
    "    p_min=(\"performance\", 'min'),\n",
    "    p_max=(\"performance\", 'max'),\n",
    "    ppn_mean=(\"performance_per_node\", 'mean'),\n",
    "    ppn_min=(\"performance_per_node\", 'min'),\n",
    "    ppn_max=(\"performance_per_node\", 'max'),\n",
    "    time_mean=(\"time\", 'mean'),\n",
    "    measures=('performance', 'count')\n",
    ")\\\n",
    ".reset_index()\n",
    "\n",
    "display(df_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_groups = df_runs.groupby(['m', 'mb'])\n",
    "\n",
    "domain_m = df['m'].unique().tolist()\n",
    "domain_mb = df['mb'].unique().tolist()\n",
    "\n",
    "domain_m.sort()\n",
    "domain_mb.sort()\n",
    "\n",
    "fig, axs_perf = plt.subplots(len(domain_mb), len(domain_m),\n",
    "                             squeeze=False, sharey='col', figsize=(9,12))\n",
    "\n",
    "for (m, mb), grp_data in df_groups:\n",
    "    ax_i, ax_j = domain_mb.index(mb), domain_m.index(m)\n",
    "    ax_perf = axs_perf[ax_i, ax_j]\n",
    "    \n",
    "    for (lib, campaign), lib_data in grp_data.groupby(['benchmark_lib', 'campaign']):\n",
    "        lib_name, lib_variant = lib.split(\"|\")\n",
    "        lib_data.plot(ax=ax_perf, x='nodes', y='ppn_mean',\n",
    "                      marker='.', linestyle = '-' if lib_variant == '1' else '--',\n",
    "                      label=f'{lib}@{campaign}', color=LIB_COLOR[lib_name])\n",
    "\n",
    "    ax_perf.set_ylim(0, 1200)\n",
    "    if ax_j == 0:\n",
    "        ax_perf.set_ylabel(f'GFlops/node\\n(mb={mb})')\n",
    "    \n",
    "    for ax in [ax_perf]:\n",
    "        ax.get_legend().remove() #loc=\"upper right\")\n",
    "        ax.set_xscale('log', basex=2)\n",
    "        ax.set_xticks([2**x for x in range(7)])\n",
    "        ax.grid(color='gray', alpha=0.3)\n",
    "        \n",
    "        if ax_i == len(domain_mb) - 1:\n",
    "            ax.set_xlabel(f'nodes')\n",
    "        if ax_i == 0:\n",
    "            ax.set_title(f'm={m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_multi = df_runs.set_index(['library', 'ranks_per_node', 'mb', 'campaign']).sort_index()\n",
    "\n",
    "display(df_runs_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually add data from HPX-prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_prototype = pd.read_csv(\n",
    "    '/Users/ialberto/workspace/benchmarks/20200317-DLAF_miniappcholesky-pre/res/resgf_hpx-mc.txt',\n",
    "    header=0,\n",
    "    index_col=False,\n",
    "    names=[\n",
    "        'm', 'mb',\n",
    "        'nodes', 'ranks_per_node', 'cores_per_task',\n",
    "        'time_mean', 'time_min',\n",
    "        'p_mean', 'p_max',\n",
    "        'ppn_mean', 'ppn_max'],\n",
    "    )\n",
    "\n",
    "df_prototype['library'] = 'prototype'\n",
    "df_prototype['benchmark_lib'] = 'prototype|1'\n",
    "df_prototype['campaign'] = 'ref'\n",
    "\n",
    "df_prototype.set_index(['library', 'ranks_per_node', 'mb', 'campaign'], inplace=True)\n",
    "df_prototype = df_prototype[df_prototype.nodes <= 2**6]\n",
    "\n",
    "df_prototype = df_prototype.drop(index=('prototype', 1, 256, 'ref')).sort_index()\n",
    "\n",
    "df_runs_multi = df_runs_multi.append(df_prototype).sort_index()\n",
    "# display(df_runs_multi[df_runs_multi.benchmark_lib == 'prototype|1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_interesting(interesting_configs, plot_time=False):\n",
    "    if type(interesting_configs) == list:\n",
    "        interesting_configs = {k:{} for k in interesting_configs}\n",
    "    \n",
    "    domain_m = df['m'].unique().tolist()\n",
    "    domain_mb = df['mb'].unique().tolist()\n",
    "\n",
    "    domain_m.sort()\n",
    "    domain_mb.sort()\n",
    "\n",
    "    fig, axs_perf = plt.subplots(len(domain_m), 1,\n",
    "                                 squeeze=False, sharey='row', figsize=(9, 12))\n",
    "    \n",
    "    if plot_time:\n",
    "        fig, axs_time = plt.subplots(len(domain_m), 1,\n",
    "                                     squeeze=False, figsize=(9, 12))\n",
    "        \n",
    "\n",
    "    for config in interesting_configs.keys():\n",
    "        plot_properties = {\n",
    "            # default properties\n",
    "            **dict(color=LIB_COLOR[config[0]], marker='.', linestyle = '-'),\n",
    "            # passed-by-argument\n",
    "            **interesting_configs[config]\n",
    "        }\n",
    "        mb = config[2]\n",
    "        for m, grp_data in df_runs_multi.loc[config, :].groupby('m'):\n",
    "            ax_i, ax_j = domain_m.index(m), 0\n",
    "            ax_perf = axs_perf[ax_i, ax_j]\n",
    "            \n",
    "            if plot_time:\n",
    "                ax_time = axs_time[ax_i, ax_j]\n",
    "\n",
    "            for (lib, campaign_name), lib_data in grp_data.groupby(['benchmark_lib', 'campaign']):\n",
    "                lib_name, lib_variant = lib.split(\"|\")\n",
    "                lib_data.plot(ax=ax_perf, x='nodes', y='ppn_mean',\n",
    "                              label=config if len(config) > 3 else f'{config} {campaign_name}',\n",
    "                              **plot_properties)\n",
    "                ax_perf.fill_between(lib_data['nodes'], lib_data['ppn_min'], lib_data['ppn_max'],\n",
    "                                     alpha=0.2, color=plot_properties['color'])\n",
    "                \n",
    "                if plot_time:\n",
    "                    lib_data.plot(ax=ax_time, x='nodes', y='time_mean',\n",
    "                                  label=config if len(config) > 3 else f'{config} {campaign_name}',\n",
    "                                  **plot_properties)\n",
    "\n",
    "            axs = [ax_perf]\n",
    "                    \n",
    "            ax_perf.set_ylabel(f'GFlops/node (with m={m})')\n",
    "            ax_perf.set_ylim(0, 1200)\n",
    "            \n",
    "            if plot_time:\n",
    "                axs.append(ax_time)\n",
    "                ax_time.set_ylabel(f'Time (s)\\n(with m={m})')\n",
    "\n",
    "            for ax in axs:\n",
    "                ax.legend(loc=\"upper right\")\n",
    "                ax.set_xscale('log', basex=2)\n",
    "                ax.set_xticks([2**x for x in range(7)])\n",
    "                ax.set_xlabel(f'nodes')\n",
    "                ax.grid(color='gray', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interesting_pres(m, interesting_configs, plot_time=False):\n",
    "    if type(interesting_configs) == list:\n",
    "        interesting_configs = {k:{} for k in interesting_configs}\n",
    "\n",
    "    fig, axs_perf = plt.subplots(1, 1,\n",
    "                                 squeeze=False, sharey='row', figsize=(4.5, 3.8))\n",
    "    \n",
    "    if plot_time:\n",
    "        fig, axs_time = plt.subplots(1, 1,\n",
    "                                     squeeze=False, figsize=(4.5, 3.8))\n",
    "        \n",
    "\n",
    "    for config in interesting_configs.keys():\n",
    "        plot_properties = {\n",
    "            # default properties\n",
    "            **dict(color=LIB_COLOR[config[0]], marker='.', linestyle = '-'),\n",
    "            # passed-by-argument\n",
    "            **interesting_configs[config]\n",
    "        }\n",
    "        mb = config[2]\n",
    "        for current_m, grp_data in df_runs_multi.loc[config, :].groupby('m'):\n",
    "            if m != current_m:\n",
    "                continue\n",
    "            \n",
    "            ax_i, ax_j = 0, 0\n",
    "            ax_perf = axs_perf[ax_i, ax_j]\n",
    "            \n",
    "            if plot_time:\n",
    "                ax_time = axs_time[ax_i, ax_j]\n",
    "\n",
    "            for (lib, campaign_name), lib_data in grp_data.groupby(['benchmark_lib', 'campaign']):\n",
    "                lib_name, lib_variant = lib.split(\"|\")\n",
    "                lib_data.plot(ax=ax_perf, x='nodes', y='ppn_mean', **plot_properties)\n",
    "                ax_perf.fill_between(lib_data['nodes'], lib_data['ppn_min'], lib_data['ppn_max'],\n",
    "                                     alpha=0.2, color=plot_properties['color'])\n",
    "                \n",
    "                if plot_time:\n",
    "                    lib_data.plot(ax=ax_time, x='nodes', y='time_mean', **plot_properties)\n",
    "\n",
    "            axs = [ax_perf]\n",
    "                    \n",
    "            ax_perf.set_ylabel(f'GFlops/node')\n",
    "            ax_perf.set_ylim(0, 1200)\n",
    "            \n",
    "            if plot_time:\n",
    "                axs.append(ax_time)\n",
    "                ax_time.set_ylabel(f'Time (s)')\n",
    "\n",
    "            for ax in axs:\n",
    "                ax.legend(loc=\"upper right\")\n",
    "                ax.set_xscale('log', basex=2)\n",
    "                ax.set_xticks([2**x for x in range(7)])\n",
    "                ax.set_xlabel(f'nodes')\n",
    "                ax.grid(color='gray', alpha=0.3)\n",
    "                ax.set_title(f'Matrix Size = {m}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for mb in [256, 512]:\n",
    "    plot_interesting({\n",
    "      ('dlaf', 2, mb, 'post'): dict(linestyle='--', color='black'),\n",
    "      ('dlaf', 2, mb,  '1.5.0'): {},\n",
    "    }, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for mb in [256, 512]:\n",
    "    for campaign_vs in ['post', '1.5.0']:\n",
    "        plot_interesting({\n",
    "          ('dlaf', 2, mb, 'ref-mar20'): dict(linestyle='--', color='black'),\n",
    "          ('dlaf', 2, mb,  campaign_vs): {},\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare libraries (best configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_interesting([\n",
    "    ('dlaf', 2, 512, 'post'),\n",
    "    ('slate', 2, 384),\n",
    "    ('dplasma', 1, 384),\n",
    "    ('scalapack', 36, 192),\n",
    "    ('prototype', 1, 512),\n",
    "], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_interesting_pres(20480, {\n",
    "    ('dlaf', 2, 512, 'post'): dict(label='DLAF'),\n",
    "    ('slate', 2, 384): dict(label='SLATE'),\n",
    "    ('dplasma', 1, 384): dict(label='DPlasma'),\n",
    "    ('scalapack', 36, 192): dict(label='ScaLAPACK (libsci)'),\n",
    "    ('prototype', 1, 512): dict(label='Prototype'),\n",
    "}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_interesting_pres(40960, {\n",
    "    ('dlaf', 2, 512, 'post'): dict(label='DLAF'),\n",
    "    ('slate', 2, 384): dict(label='SLATE'),\n",
    "    ('dplasma', 1, 384): dict(label='DPlasma'),\n",
    "    ('scalapack', 36, 192): dict(label='ScaLAPACK (libsci)'),\n",
    "    ('prototype', 1, 512): dict(label='Prototype'),\n",
    "}, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocksizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MB_COLORS = [\n",
    "    dict(color='red'),\n",
    "    dict(color='orange'),\n",
    "    dict(color='yellow'),\n",
    "    dict(color='green'),\n",
    "    dict(color='cyan'),\n",
    "]\n",
    "\n",
    "def plot_mbs(mbs_library):\n",
    "    plot_interesting({k:v for k,v in zip(mbs_library, MB_COLORS)}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for rank_per_node in [1,2]:\n",
    "    mbs = [('dlaf', rank_per_node, mb, 'post') for mb in df[df['library'] == 'dlaf'].mb.sort_values().unique().tolist()]\n",
    "    plot_mbs(mbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mbs = [('dplasma', 1, mb) for mb in df[df['library'] == 'dplasma'].mb.sort_values().unique().tolist()]\n",
    "\n",
    "plot_mbs(mbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for rank_per_node in [1,2]:\n",
    "    mbs = [('slate', rank_per_node, mb) for mb in df[df['library'] == 'slate'].mb.sort_values().unique().tolist()]\n",
    "    plot_mbs(mbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mbs = [('scalapack', 36, mb) for mb in df[df['library'] == 'scalapack'].mb.sort_values().unique().tolist()]\n",
    "\n",
    "plot_mbs(mbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison library configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interesting({\n",
    "    ('dlaf', 1, 512, 'post'): {},\n",
    "    ('dlaf', 2, 512, 'post'): dict(linestyle='--'),\n",
    "}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_interesting({\n",
    "    ('slate', 1, 384, 'post'): {},\n",
    "    ('slate', 2, 384, 'post'): dict(linestyle='--'),\n",
    "}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
